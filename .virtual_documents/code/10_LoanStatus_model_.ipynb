# Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


ppp = pd.read_csv('../data/cleaned_ppp.csv')
ppp.head()





num_code_features = ['ServicingLenderLocationID', 'NAICSCode', 'OriginatingLenderLocationID', 'CD', 'BorrowerZip']

for p in num_code_features:
    ppp[p] = ppp[p].astype(str)


ppp.dtypes


# Define X and y
X = ppp.drop(columns=['LoanStatus'])
y = ppp['LoanStatus']


X.shape[1]





print(f'{round((1 / np.sqrt(X.shape[1])*100), 2)}% should be reserved for validation')


print(f'{round(((1 - 1 / np.sqrt(X.shape[1]))*100), 2)}% should be used for training')


baseline_accuracy = y.value_counts(normalize=True)[1]
y.value_counts(normalize=True)





# Make a list of categorical features
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(exclude=['object']).columns.tolist()


# len(ppp['BusinessType'].unique())


from sklearn.base import BaseEstimator, TransformerMixin

class ColumnSelector(BaseEstimator, TransformerMixin):
    def __init__(self, columns):
        self.columns = columns
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        return X[self.columns]


# Preprocessing pipeline for categorical features
preprocessor = ColumnTransformer(transformers=[
    ('oh', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
    ('num', StandardScaler(), numerical_cols),
   # ('selector', ColumnSelector(columns=~df.columns.isin(columns_to_drop)), slice(None)),
])

## TODO stop here, it's too much work to reshape the categorical values


# Define the Random Forest model with preprocessing and classifier
# Use default tuning parameters
rf = RandomForestClassifier(n_estimators=5, max_depth=3, )

pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('feature_selector', SelectColumnsTransFormer
    ('rf', rf)
])


# Perform train test split
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.1, random_state=42)


# Split training set further into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)


pipe.fit(X_train, y_train)


pipe.score(X_train, y_train)


pipe.score(X_val, y_val)


# Delete this! Just peeking at holdout data
# pipe.score(X_test, y_test) # 0.9108





# Reassemble the feature names so that we can map these correctly to feature importances

transformed_feature_names = pipe.named_steps['preprocessor'].named_transformers_['oh']      .get_feature_names_out(categorical_cols).tolist()
all_feature_names = transformed_feature_names + numerical_cols
len(all_feature_names)


# Get feature importances from the trained model
feature_importances = pipe.named_steps['rf'].feature_importances_
len(feature_importances)


# Create a DataFrame to display feature importances
importance_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importances})
sorted_importances = importance_df.sort_values(by='Importance', ascending=False)
top_features = sorted_importances[sorted_importances['Importance'] > 0]
np.round(top_features, 4)











def extract_name(f):
    return f.split('_')[0]
def extract_cat(f):
    return f.split('_')[1]


extraction_df = pd.DataFrame(sorted_importances).drop(columns=['Feature'])
extraction_df['TopCat'] = top_features['Feature'].apply(extract_name)
extraction_df['SubCat'] = top_features['Feature'].apply(extract_cat)
extraction_df = extraction_df.reset_index(drop=True) # resets index after sorting for ease
# extraction_df.shape


extraction_df.iloc[:10,:]


# Originating lenders top 10
top_orig_lend_ID = extraction_df[extraction_df['TopCat'] == 'OriginatingLenderLocationID'].sort_values(by='Importance', ascending=False).iloc[:10,:]

top_orig_lend_ID 





# Servicing lenders top 10
top_serv_lend_ID = extraction_df[extraction_df['TopCat'] == 'ServicingLenderLocationID'].sort_values(by='Importance', ascending=False).iloc[:10,:]

top_serv_lend_ID


# BorrowerZip top 10
top_borr_zip = extraction_df[extraction_df['TopCat'] == 'BorrowerZip'].sort_values(by='Importance', ascending=False).iloc[:10,:]

top_borr_zip


extraction_df.iloc[10:20,:]


# NAICSCode top 10
top_NAICS = extraction_df[extraction_df['TopCat'] == 'NAICSCode'].sort_values(by='Importance', ascending=False).iloc[:10,:]

top_NAICS























# Calculate precision, recall, and F1-score for each class
precision = cm[1, 1] / (cm[1, 1] + cm[0, 1])
# recall = cm.diagonal() / (cm.sum(axis=1) + 1)
# f1_score = 2 * (precision * recall) / (precision + recall)


print(f"Overall Accuracy: {overall_accuracy:.4f}")
print(f"Baseline Accuracy: {baseline_accuracy:.4f}")
print(f"Precision: {precision:.4f}")
# print(f"Recall (Sensitivity): {recall}")
# print(f"F1-Score: {f1_score}")


# Compute overall accuracy from confusion matrix
overall_accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()


ConfusionMatrixDisplay.from_estimator(pipe, X_val, y_val, cmap='Blues');


# Examine y predictions
y_pred = pipe.predict(X_val)
pd.Series(y_pred).value_counts(normalize=True)


# Examine the confusion matrix
cm = confusion_matrix(y_val, y_pred)


print(cm)







































