





# Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# dataset sourced from https://data.sba.gov/dataset/ppp-foia
# created March 2, 2021, 4:30 PM (UTC-08:00)
# updated October 3, 2023, 8:04 AM (UTC-07:00)
# split across 13 csv files - 12 files describe loans under 150k, 1 file describe loans above 150k

ppp_1 = pd.read_csv('../data/public_up_to_150k_1_230930.csv')


ppp_1.head()


ppp_1.info()





# Generate an easy to read data dictionary
dict_df = pd.read_csv('../data/ppp-data-dictionary.csv', index_col=0)
dict_df['dtype'] = ppp_1.dtypes
dict_df.reset_index(inplace=True)


dict_df.iloc[:5, :]





dict_df.iloc[3,1] # Greater detail on ProcessingMethod











dict_df.iloc[5:10,:]





dict_df.iloc[9, 1]





dict_df.iloc[10:15,:]


dict_df.iloc[10, 1]








dict_df.iloc[15:23,:]





dict_df.iloc[23:31,:]





dict_df.iloc[31:43,:]





dict_df.iloc[36,1]





dict_df.iloc[43:,:]








# TODO make a pipline to process all the separate csvs this way


ppp_1['DateApproved'] = pd.to_datetime(ppp_1['DateApproved'])
ppp_1['DateApproved'].head()


ppp_1['LoanNumber'].head()


ppp_1['SBAOfficeCode'].describe()


ppp_1['ProcessingMethod'].describe()


ppp_1['BorrowerName'].unique()


ppp_1['LoanStatusDate'] = pd.to_datetime(ppp_1['LoanStatusDate'])
ppp_1[['LoanStatusDate', 'LoanStatus']].head()


# ppp_1 = pd.read_csv('../data/public_up_to_150k_1_230930.csv')
ppp_1['LoanStatusDate'].value_counts()


np.sum(ppp_1['LoanStatusDate'].isna()) / ppp_1['LoanStatusDate'].size





ppp_1[ppp_1['LoanStatusDate'].isna()][['BusinessType', 'BorrowerName', 'CurrentApprovalAmount']]


ppp_1['LoanStatus'].value_counts()


ppp_1[ppp_1['LoanStatus'].isna()]


ppp_1[['SBAGuarantyPercentage', 'Term']].describe()


loan_sz_inc = ppp_1[ppp_1['InitialApprovalAmount'] < ppp_1['CurrentApprovalAmount']]


# By how much were loan amounts increased?
percent_inc = (loan_sz_inc['CurrentApprovalAmount'] - loan_sz_inc['InitialApprovalAmount'])/loan_sz_inc['InitialApprovalAmount']


percent_inc.describe()





ppp_1['UndisbursedAmount'].describe()


np.sum(ppp_1['UndisbursedAmount'] > 0)  # only one loan was not fully disbursed


ppp_1[ppp_1['FranchiseName'].notna()]['FranchiseName'].head(10)





serv_orig_lender = ppp_1[['CurrentApprovalAmount', 'ServicingLenderName', 'OriginatingLender']]
diff_lenders_mask = (ppp_1['ServicingLenderName'] != ppp_1['OriginatingLender']) & ppp_1['OriginatingLender'].notna()
print(serv_orig_lender[diff_lenders_mask]['OriginatingLender'].value_counts()[:10])
print(serv_orig_lender[diff_lenders_mask]['ServicingLenderName'].value_counts()[:10])





ppp_1['BusinessType'].value_counts()





np.sum(ppp_1['RuralUrbanIndicator'].isna())


print(np.sum(ppp_1['HubzoneIndicator'].isna()))





ppp_1['BusinessAgeDescription'].head()


ppp_1['BusinessAgeDescription'].value_counts()





# TODO decide how to best encode business age


np.sum(ppp_1['JobsReported'].isna()) # One business does not report their number of employees


ppp_1.where(ppp_1['JobsReported'].notna(), other=0, inplace=True)


ppp_1['JobsReported'].astype('int32')


ppp_1['NAICSCode'].value_counts()








import sweetviz as sv


ppp_1['DateApproved'].value_counts()


ppp_1['DateApproved'] = pd.to_datetime(ppp_1['DateApproved'])


ppp_1['DateApproved'].info()


ppp_1['ProcessingMethod'].info()


ppp_1['ProcessingMethod'] = ppp_1['ProcessingMethod'].astype(str)


ppp_1['ProcessingMethod'].info()


ppp_1['BorrowerName'] = ppp_1['BorrowerName'].astype(str)


ppp_1['BorrowerAddress'] = ppp_1['BorrowerAddress'].astype(str)
ppp_1['BorrowerZip'] = ppp_1['BorrowerZip'].astype(str)



ppp_1['LoanStatusDate'] = pd.to_datetime(ppp_1['LoanStatusDate'])


ppp_1['LoanStatus'] = ppp_1['LoanStatus'].astype(str)


ppp_1['FranchiseName'] = ppp_1['FranchiseName'].astype(str)


ppp_1['ServicingLenderLocationID'] = ppp_1['ServicingLenderLocationID'].astype(str)


ppp_1['ServicingLenderAddress'] = ppp_1['ServicingLenderAddress'].astype(str)
ppp_1['ServicingLenderZip'] = ppp_1['ServicingLenderZip'].astype(str)

ppp_1['BusinessAgeDescription'] = ppp_1['BusinessAgeDescription'].astype(str)

ppp_1['ProjectZip'] = ppp_1['ProjectZip'].astype(str)


ppp_1['NAICSCode'] = ppp_1['NAICSCode'].astype(str)


ppp_1.loc[ppp_1['UTILITIES_PROCEED'].isna(), 'UTILITIES_PROCEED'] = 0


temp_df = pd.read_csv('../data/public_up_to_150k_1_230930.csv')


ppp_1['UTILITIES_PROCEED'] = temp_df['UTILITIES_PROCEED']


np.sum(ppp_1['UTILITIES_PROCEED'].isna())


ppp_1['UTILITIES_PROCEED'].value_counts()


ppp_1['PAYROLL_PROCEED'].value_counts()


ppp_1.loc[ppp_1['UTILITIES_PROCEED'].isna()][['CurrentApprovalAmount', 'PAYROLL_PROCEED', 'BorrowerName']].head(20)





# Map nulls to $0.00 
ppp_1.loc[ppp_1['UTILITIES_PROCEED'].isna(), 'UTILITIES_PROCEED'] = 0.0


np.sum(ppp_1['UTILITIES_PROCEED'].isna())


ppp_1['UTILITIES_PROCEED'] = ppp_1['UTILITIES_PROCEED'].astype(float)


# Repeat a similar mapping for other Proceed categories
for category in ['UTILITIES_PROCEED', 'PAYROLL_PROCEED', 'MORTGAGE_INTEREST_PROCEED', 'RENT_PROCEED', 'REFINANCE_EIDL_PROCEED', 'HEALTH_CARE_PROCEED', 'DEBT_INTEREST_PROCEED']:
    ppp_1.loc[ppp_1[category].isna(), category] = 0.0
    ppp_1[category] = ppp_1[category].astype(float)


cast_to_str = ['BorrowerState', 'ServicingLenderName', 'ServicingLenderCity','ServicingLenderState', 'ServicingLenderZip', 'RuralUrbanIndicator', 'HubzoneIndicator', 'LMIIndicator', 'BusinessAgeDescription', 'ProjectCity', 'ProjectState', 'ProjectZip', 'CD', 'Ethnicity', 'OriginatingLender', 'OriginatingLenderCity', 'OriginatingLenderState']

for el in cast_to_str:
    ppp_1[el] = ppp_1[el].astype(str)


# One incorrectly recorded value for race as an integer zero, which we surmise is unanswered.
ppp_1['Race'].value_counts()


ppp_1['Race'] = ppp_1['Race'].replace(to_replace=0, value='Unanswered')
ppp_1[ppp_1['Race'] == 0]


ppp_1['Ethnicity'].value_counts()


ppp_1['Ethnicity'] = ppp_1['Ethnicity'].replace(to_replace='0', value='Unknown/NotStated')


ppp_1['Ethnicity'].value_counts()


ppp_1['Gender'] = ppp_1['Gender'].replace(to_replace=0, value='Unanswered')
ppp_1['Gender'].value_counts()


ppp_1['Veteran'] = ppp_1['Veteran'].replace(to_replace=0, value='Unanswered')
ppp_1['Veteran'].value_counts()


ppp_1['NonProfit'].value_counts()

ppp_1[ppp_1['NonProfit']==0].to_dict()





# The following list is the columns which struggle to coerce to str values
# error_cols = ['BorrowerCity', 'ProjectCountyName', 'ProjectState', 'CD']

# ppp_1_dropped = ppp_1.drop(columns=error_cols)

error_idx = ppp_1[ppp_1['NonProfit']==0].index # Index([580847], dtype='int64')
ppp_1 = ppp_1.drop(index=error_idx)


report_1 = sv.analyze(ppp_1)
report_1.show_html()





ppp_1_ls = pd.get_dummies(data=ppp_1, columns=['LoanStatus'])


ppp_1_ls.columns


report_1_regression = sv.analyze(ppp_1_ls, target_feat='LoanStatus_Exemption 4')
report_1_regression.show_html()


report_2_regression = sv.analyze(ppp_1_ls, target_feat='LoanStatus_Charged Off')
report_2_regression.show_html()


report_3_regression = sv.analyze(ppp_1_ls, target_feat='LoanStatus_Paid in Full')
report_3_regression.show_html()








#  Here! TODO
# Consult how to handle csv data in Git
# Decide how to handle merging all the csvs
# Create a classification model {what target var} (consult J. Case)
# Create a regression model {what target var} (consult J. Case)
